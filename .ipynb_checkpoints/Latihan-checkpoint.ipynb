{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #import library nltk\n",
    "from nltk.tokenize import word_tokenize #import word_tokenize for tokenizing text into words \n",
    "from nltk.tokenize import sent_tokenize #import sent_tokenize for tokenizing paragraph into sentences\n",
    "from nltk.stem.porter import PorterStemmer #import Porter Stemmer Algorithm \n",
    "from nltk.stem import WordNetLemmatizer #import WordNet lemmatizer \n",
    "from nltk.corpus import stopwords #import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #import Indonesian Stemmer\n",
    "import re #import regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saya suka belajar.',\n",
       " 'Karena ingin menjadi pintar.',\n",
       " 'Selain itu, saya ingin membuat bahagia kedua orang tua.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "def sentence_tokenization(s): #mendefinisikan sebuah fungsi , diikuti dengan ():\n",
    "    sentences_list = sent_tokenize(s) #penggunaan tab untuk child, membuat variabel yang berisi sent_tokenize yang berfungsi untuk mengubah pragraf menjadi beberapa kalimat\n",
    "    \n",
    "    return sentences_list \n",
    "\n",
    "text_data = \"Saya suka belajar. Karena ingin menjadi pintar. Selain itu, saya ingin membuat bahagia kedua orang tua.\"\n",
    "sentence_tokenization(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sofia\n",
      "[nltk_data]     Dewi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saya',\n",
       " 'suka',\n",
       " 'belajar',\n",
       " '.',\n",
       " 'Karena',\n",
       " 'ingin',\n",
       " 'menjadi',\n",
       " 'pintar',\n",
       " '.',\n",
       " 'Selain',\n",
       " 'itu',\n",
       " ',',\n",
       " 'saya',\n",
       " 'ingin',\n",
       " 'membuat',\n",
       " 'bahagia',\n",
       " 'kedua',\n",
       " 'orang',\n",
       " 'tua',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#word tokenization\n",
    "def word_tokenization(s):\n",
    "    tokens = word_tokenize(s)\n",
    "\n",
    "    return tokens\n",
    "    \n",
    "text_data = \"Saya suka belajar. Karena ingin menjadi pintar. Selain itu, saya ingin membuat bahagia kedua orang tua.\"\n",
    "word_tokenization(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saya suka belajar. karena ingin menjadi pintar. selain itu, saya ingin membuat bahagia kedua orang tua.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#casefolding\n",
    "def casefolding(s):\n",
    "    new_str = s.lower()\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "text_data = \"Saya suka belajar. Karena ingin menjadi pintar. Selain itu, saya ingin membuat bahagia kedua orang tua.\"\n",
    "casefolding(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saya suka ajar karena ingin jadi pintar selain itu saya ingin buat bahagia dua orang tua'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming Indonesian\n",
    "def stemmingIndo(str):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(str)\n",
    "\n",
    "text_data = \"Saya suka belajar. Karena ingin menjadi pintar. Selain itu, saya ingin membuat bahagia kedua orang tua.\"\n",
    "stemmingIndo(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she had been with her father and sister when she wa attack and receiv first aid at the scene , an offici said .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming English\n",
    "def stemmingEnglish(str):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = word_tokenize(str)\n",
    "    result = list()\n",
    "    for word in words:\n",
    "        result.append(porter_stemmer.stem(word))\n",
    "        \n",
    "    return ' '.join(result)\n",
    "\n",
    "text_data = \"She had been with her father and sister when she was attacked and received first aid at the scene, an official said.\"\n",
    "stemmingEnglish(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Stem: It\n",
      "Actual: originated  Stem: origin\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: idea  Stem: idea\n",
      "Actual: that  Stem: that\n",
      "Actual: there  Stem: there\n",
      "Actual: are  Stem: are\n",
      "Actual: readers  Stem: reader\n",
      "Actual: who  Stem: who\n",
      "Actual: prefer  Stem: prefer\n",
      "Actual: learning  Stem: learn\n",
      "Actual: new  Stem: new\n",
      "Actual: skills  Stem: skill\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: comforts  Stem: comfort\n",
      "Actual: of  Stem: of\n",
      "Actual: their  Stem: their\n",
      "Actual: drawing  Stem: draw\n",
      "Actual: rooms  Stem: room\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "for w in nltk_tokens:\n",
    "       print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Lemma: It\n",
      "Actual: originated  Lemma: originated\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: idea  Lemma: idea\n",
      "Actual: that  Lemma: that\n",
      "Actual: there  Lemma: there\n",
      "Actual: are  Lemma: are\n",
      "Actual: readers  Lemma: reader\n",
      "Actual: who  Lemma: who\n",
      "Actual: prefer  Lemma: prefer\n",
      "Actual: learning  Lemma: learning\n",
      "Actual: new  Lemma: new\n",
      "Actual: skills  Lemma: skill\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: comforts  Lemma: comfort\n",
      "Actual: of  Lemma: of\n",
      "Actual: their  Lemma: their\n",
      "Actual: drawing  Lemma: drawing\n",
      "Actual: rooms  Lemma: room\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "       print (\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sofia\n",
      "[nltk_data]     Dewi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saya lahir   tanggal   Januari     '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove digit from string\n",
    "def removeDigit(str):\n",
    "    new_string =  re.sub(r\"[\\W,0-9]\", \" \", str)\n",
    "    return new_string\n",
    "\n",
    "text_data = \"saya lahir:\\ tanggal 1 Januari 2016\"\n",
    "removeDigit(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('originated', 'VBD'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('idea', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('there', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('readers', 'NNS'),\n",
       " ('who', 'WP'),\n",
       " ('prefer', 'VBP'),\n",
       " ('learning', 'VBG'),\n",
       " ('new', 'JJ'),\n",
       " ('skills', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('comforts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('drawing', 'NN'),\n",
       " ('rooms', 'NNS')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos tagging\n",
    "def postag(str):\n",
    "    tok_sentence = nltk.word_tokenize(str)\n",
    "    tagged_sentence = nltk.pos_tag(tok_sentence)\n",
    "    return tagged_sentence\n",
    "\n",
    "text_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "postag(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sofia Dewi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand Indonesia yang Lolos di bazar Maison&Objet, Paris\n",
      "\n",
      "\n",
      "\n",
      "Alvin Tjitrowirjo mempertunjukkan desain produknya dibawah brand miliknya yang diberi nama alvinT pada event yang akan datang, Maison&Objet, sebuah bazar gaya hidup yang akan diadakan di Paris-Nord Villepinte convention center, Paris, di mulai pada tanggal 7 kemarin hingga 11 September mendatang.\n",
      "\n",
      "Dari benda-benda yang akan dipertunjukkan oleh alvinT adalah kuda kayu besar, kursi Malya, Loop stool, dan bangku. Semua benda ini dibuat dari jenis baru rotan bernama karuun, dikembangkan oleh perusahaan manufaktur asal Jerman.\n",
      "\n",
      "\"Karuun sesungguhnya merupakan sebuah teknik mewarnai baru untuk rotan. Biasanya rotan disemprot menggunakan zat pewarna dari luar, namun (karuun) semacam disuntikkan dengan warna, jadi saat dipotong kita bisa melihat di bagian dalamnya juga berwarna,\" ucap Alvin seperti yang dikutip dari Jakarta Post.\n",
      "\n",
      "Alvin menyebutkan bahwa teknik mewarnai baru tersebut telah dipatenkan dan perusahaannya memastikan bahwa rotan tersebut berkelanjutan berasal dari Aceh dan Kalimantan.\n",
      "\n",
      "\"Sesungguhnya, nama karuun merujuk pada materialnya,\" ucap Alvin, \"dan saya mendesain (produknya) menggunakan material tersebut.\"\n",
      "\n",
      "Produk yang didesain oleh Alvin memiliki warna pastel, bertujuan untuk tampilan kontemporer.\n",
      "\n",
      "Moment ini merupakan ketiga kalinya Alvin menunjukkan desainnya di Maison&Objet, dimana ia berbagi booth N20 di Hall 5A dengan desainer tekstil berkebangsaan Belanda Nikkie Wester. Pada Januari lalu, Alvin menunjukkan desainnya di Maison&Objet, di booth rotan Yamakawa.\n",
      "\n",
      "Sebelumnya Alvin berkolaborasi dengan brand fashion Indonesia, Sejauh Mata Memandang untuk dipertunjukkan di Maison&Objet 2017.\n",
      "\n",
      "Alvin menyatakan bahwa untuk dapat menunjukkan hasil karyanya di ajang prestis seperti ini bukanlah hal yang mudah, ia harus melewati proses seleksi yang cukup menantang.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main(): \n",
    "    file = open ( \"bahasa.txt\" ) \n",
    "    lines = file.readlines() \n",
    "    file.close() \n",
    "     \n",
    "    for line in lines: \n",
    "        print( line ) \n",
    "         \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
